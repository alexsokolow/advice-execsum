{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db40e37a-ba5f-4089-bcc0-de4ccbea3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from llama_index import download_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebe1482-8159-43ad-af51-81e582849169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/alexandresokolow/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q4_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q4_K     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name   = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 4165.47 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8000\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: kv self size  = 1000.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 740/740\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Max\n",
      "ggml_metal_init: picking default device: Apple M1 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: loading '/Users/alexandresokolow/code/advice-llm/venv/lib/python3.8/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 49152.00 MiB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 542.69 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  4166.08 MiB, ( 4166.58 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1000.02 MiB, ( 5166.59 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   539.64 MiB, ( 5706.23 / 49152.00)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "        model_path=\"/Users/alexandresokolow/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "        temperature=0.001,\n",
    "        top_p=0.95,\n",
    "        top_k=1,\n",
    "        n_ctx=8000,\n",
    "        max_tokens=2048,\n",
    "        n_gpu_layers=1,\n",
    "        n_batch=512,\n",
    "        seed=27,\n",
    "        f16_kv=True,\n",
    "        verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15bf75-1275-402b-bc60-417272975820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(\"pdf/tank/PR3016700_tank_report.pdf\")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca52d5e-8cb4-4463-8ca3-08cb655c8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### GENERATE SUMMARY OF EACH PART\n",
    "\n",
    "chapter_ = documents[4].page_content + '\\n' + documents[5].page_content\n",
    "print(chapter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c0bd6-00d2-42ea-83d2-7684a2b97790",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_SUMMARY_CHAPTER_STR = (\n",
    "    \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context. The following is a part of report:\n",
    "\n",
    "{context}\n",
    "\n",
    "Based on this, please identify the main points, written in a professional tone suitable for an academic audience. [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd173a-9850-44a2-8b79-084c84179b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_template = \"\"\"<s>[INST] The following is a part of report:\n",
    "\n",
    "{docs}\n",
    "\n",
    "Based on this, please identify the main points, written in a professional tone suitable for an academic audience. [/INST]\"\"\"\n",
    "#Answer: \n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b418fd-3435-4804-978d-1d306b5bd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "### only description of the event (manual cleaning)\n",
    "\n",
    "descr_event = \"\"\"2. DESCRIPTION OF THE EVENT\n",
    "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
    "Monitoring (EM) are as indicated in Table 1. \n",
    "PR# Description Sample date Discovery Date Results \n",
    "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
    "Table 1 Event description\n",
    "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
    "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
    "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
    "the specified limits (see Table 2).\n",
    "DIW \n",
    "pre-rinsing\n",
    "Alkaline \n",
    "Cleaning\n",
    "WFI \n",
    "rinsing\n",
    "\n",
    "Test Unit Result Acceptance criteria\n",
    "pH - 5.6 5.0 – 7.0\n",
    "Conductivity µS/cm 1.1 See SOP-026359\n",
    "TOC ppb 505 ≤ 500\n",
    "LAL EU/ml 0.005 ≤ 0.25\n",
    "Bioburden Cfu/10 ml 0 < 1 \n",
    "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
    "(ref.: NWA database). \n",
    "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
    "obtained were satisfactory.\n",
    "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
    "was satisfactory.\n",
    " \n",
    "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
    "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
    "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
    "PR3016700 attached in TW8).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2ceda-5872-4fba-a9f2-1128ca852501",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_00 = \"\"\"5. ROOT CAUSE INVESTIGATION\n",
    "An investigation was conducted according to 6M methodology in order to identify the root cause. Each step concerning equipment cleaning, sampling and testing will be further investigated.\n",
    "An OoL TOC on a tank could be caused by :\n",
    "    - Presence of residue of carbon residue from the lot previously produced, due to a failed CIP\n",
    "    - Contamination of the tank through the water used for CIP (WFI C102)\n",
    "    - Contamination of the tank through the water used for pH-meter flush and sanitization (WFI C104)\n",
    "    - Contamination of the tank following a maintenance activity\n",
    "    - Contamination of the sample during sampling, the sampling point or the sampling equipment\n",
    "    - Testing error\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebed43b-89b6-4343-afe1-210a4cfdc023",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_CIP = \"\"\"5.1. ENVIRONMENT\n",
    "5.1.1. Analysis of CIP cycle\n",
    "T22 is a non-protein, buffer tank in Purification L2 Post-Nano area and is used during the manufacturing of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
    "T22 CIP cycle was performed and achieved on 12 September 2022 at 17h00 prior to EM sampling performed on 12 September 2022 around 17h21. The CIP was launched after an idle time following the production of Cuvitru lot BE13C095Z (end of production on the 09 September 2022), and before the production of Cuvitru lot BE13C097Z.\n",
    "The CIP In-line conductivity (ref.: OSIPI database) results before (12 September 2022) and after EM sampling (14 September 2022) were satisfactory (1.40 and 1.31 µS/cm, respectively - ≤ 5.0 µS/cm (Ref.: SOP-013469)).\n",
    "The CIP cycle of T22 followed the CIP 638-7 sequence as detailed in IT specification (Ref.: B2382_P_D_03_7.0_Design Specification - CIP Buffer & CIP UF & SOP-054471), as shown in Table 3.\n",
    "The CIP lasted about three hours and 30 minutes. This is longer than what is usually observed when compared with other CIP cycles of T22 Tank in OSIPI database (up to 1h30 minutes). After the cleaning base step, the tank T22 is drained. When T22 weight is below 10kg, the final rinsing step is started. On 12 September 2022, after the cleaning base step, T22 weight could not get below 11kg. Therefore, the final rinsing step could not be started which caused the longer CIP time. However, the cleaning base and final rinsing steps duration were usual and were compliant. There is no reason to think that this longer CIP time would be the cause of an OoL TOC or a failed cleaning.\n",
    "No alarms were reported during this CIP.\n",
    "The condition of \"NaOH Injection (Conductivity CE49561> 89,4 mS/cm during 40 s)\" and circulation of cleaning base solution during 15 minutes in T22 tank as well as final rinsing step were held correctly as required by ITS sequence (see Figures 3 & 4).\n",
    "The final CIP In-line conductivity measured for the CIP cycle of T22 tank performed on 12 September 2022 was 1.40 µS/cm which is below the limit validated to release a CIP cycle for routine manufacturing, i.e. ≤ 5.0 µS/cm (Ref.: SOP-013469). Additionally, the trend of T22 tank final CIP In-line conductivity (11 August 2022 – 21 September 2022), measured after the CIP of T22 did not show any negative tendency, (see Figure 5).\n",
    "The injection of NaOH for station CIP638-7 is controlled via conductivity probe LECE49561 and final CIP In-line conductivity via conductivity probe LECE49549. These conductivity probes were verified by the metrology on, respectively, 10 September 2022 & 08 September 2022, and it was satisfactory (see Figures 8 & 9). The next due dates for these conductivity probes are, respectively, 10 March 2023 & 08 March 2023. No non-conformance events concerning these conductivity probes were reported.\n",
    "The temperature of tank circulation C2 is controlled via temperature probe LETT49573. This conductivity probe was verified by metrology on 08 September 2022, and it was satisfactory (see Figure 10). The next due date for this probe is 08 March 2023. No non-conformance event concerning this probe was reported.\n",
    "Tanks P13, P12 and Pipe TP653-TP605A in area R605 are cleaned in place as well by CIP638-7 station. These tanks are compliant for TOC for the year before the OoL TOC on T22 (from 12 September 2021 to 23 September 2022), see Figure.\n",
    "CIP 638-7 station is connected to WFI loop 102. WFI loop C102 is also used for the EM sampling of tank T22. WFI loop C102 TOC was checked for the period of 01 August 2022 till 26 September 2022. NWA data demonstrated that TOC parameters are satisfactory (see Figure 13), i.e. not more than (NMT) 500 ppb according to SOP-026359.\n",
    "Based on the considerations above, it can be concluded that the CIP cycle of T22 tank performed on 12 September 2022 was valid and the final conductivity of T22 measured in-line was 1.98 µS/cm (ref.: OSIPI database) which is below the limit validated to release a CIP cycle for routine manufacturing, i.e. ≤ 5.0 µS/cm (see Figure 5). A potential cross-contamination is therefore excluded.\n",
    "WFI loop C104 is used for sanitization and flush of pH-meters used on tanks in the R605 area. WFI loop C104 TOC was checked for the period of 01 August 2022 till 26 September 2022. NWA data demonstrated that TOC parameters are satisfactory (see Figure 14).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6131d5d8-fa3b-42ed-98c4-be439f34a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_sampling = \"\"\"5.1.2. Analysis of the sampling\n",
    "Based on OSIPI data, EM samples were taken on 12 September 2022 at about 17h21 after the CIP cycle of T22 tank, completed on 12 September 2022 at 17h00. No activities were reported in T22 between end CIP and EM sampling (see Figure 8). It can be noticed that the sampling was performed within 48h (which is the validated clean hold time) after the end of the CIP cycle as per SOP-026466.\n",
    "The EM sampling was performed according to SOP-026466 as following:\n",
    "At Transfer panel (TP) TP605A, connect elbow « TP605A-4 » from « WFI » to «In T22 » - Fill up T22 with WFI (set-point = 45 Kg) - At Transfer panel TP605, connect elbow « 605-Pré-T22 » at « Out T22 » - Start the sampling function on HMI by selecting command \"PRELEV\" to open the tank bottom valve and take EM cleaning samples at transfer panel TP605C\n",
    "In addition to EM sampling, the flexible TP605A-4 is only used to fill the tanks of the R605 area with WFI. The flexible “605-Pré-T22” is only used for EM sampling of T22 tank.\n",
    "Cleaning status of small material single elbow “605-Pré-T22” is ensured by manual cleaning. Material was cleaned as per SOP- 048680 “LE09NE03031 - NETTOYAGE MANUEL DU MATERIEL”(cfr LE99FLNE352 – BE13C097Z).\n",
    "It can be concluded that sampling is not the root cause of the out-of-limit TOC value measured on EM monitoring sample of T22 taken on 12 September 2022.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c88d7-0d2d-4e43-8fb2-ba478ab01879",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_testing = \"\"\"5.1.3. Analysis of the Testing\n",
    "The laboratory investigation was performed using the 6M methodology. No potential root cause was identified for PR3016700 (ref.: PR2996517). All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within the specified limits (see Table 2).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba50d8-9ddc-460b-b466-8c92ebb8bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_material_machine_equipement = \"\"\"5.2. MATERIAL / MACHINE / EQUIPMENT\n",
    "A review of the maintenance activities on the basis of JDE database was performed on T22, WFI loops C102 & C104, CIP station 638-7 and room 605 (from 11 August 2022 to 21 September 2022).\n",
    "This review of maintenance activities assessed (ref.: Annexe 1 attached PR3016700 in TW8) that T22, WFI loops C102 & C104, CIP station 638-7 and room 605 were fully operational and that their integrity was ensured for the EM sampling. No potential root cause was identified.\n",
    "It can be concluded that Material / Machine / Equipment is not the root cause of PR3016700.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d501aff-6113-43c3-9342-23748aaf8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_rawmaterials = \"\"\"5.3. RAW MATERIALS\n",
    "NaOH 29% is the cleaning agent used for CIP of T22 tank. No carbon in that composition.\n",
    "The TOC tube used to sample tank T22 on 12 September 2022 corresponds to lot number 21277- 4627. This lot number has been quarantined on 14 October 2022 as part of PR3060455 - Atypical trend of TOC occurrence for EM samples WFI, tanks, UF and columns from 02/Oct/22 to 07/Oct/22. This PR is still under investigation at the time.\n",
    "It can be concluded that raw materials is not the root cause of PR3016700. Nonetheless, deviation PR3060455 has been opened on 14 October 2022 for an atypical trend in the occurrence OoL TOC observed for EM samples. This PR is still under investigation at the time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d84bb-611a-4734-9b3d-b109e3120246",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_manMethod = \"\"\"5.4. MAN & METHOD\n",
    "The laboratory investigation was performed using the 6M methodology. No potential root cause was identified for PR3016700 (ref.: PR2996517). Note that all others testing performed on EM cleaning samples, i.e. pH, conductivity, LAL and bioburden were within the specified limits (see Table 2). According to the laboratory investigation (ref.: PR2996517), the EM operator and QC analyst involved in the OoL treated here were in order of training : - EM sampling was performed as required in SOP-026466 “Prelevements des eaux de rincages des tanks”\n",
    "The tests were performed as described in SOP-015413 “ Measurement of Total Organic Carbon with the use of TOC Sievers”.\n",
    "An analysis of the above mentioned SOP’s revealed that they are explicit, as the methodology to be followed for the collection of rinse samples and their measurement.\n",
    "It can be concluded that Man is not the root cause of PR3016700.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf1123-577d-4320-a1ca-e666a429eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_conform = \"\"\"5.5. CHANGE & NON-CONFORMANCE EVENTS REVIEW\n",
    "The change & non-conformance events (ref.: Trackwise 8) review was performed on the period from 11 August 2022 (last EM testing satisfactory) to 21 September 2022 (satisfactory retest of EM sampling). The research criteria applied were defined as following: “Zone 605”, “Tank T22”, “CIP station 638 7”, WFI loop C102 & C104”. No other non-conformance events were reported for T22 tank and CIP station 638-7 for the revised here period based on TW8 database. No change regarding research criteria was implemented for the revised here period based on TW8 database. The review of the maintenance activities performed on T22 between 11 August 2022 and 12 September 2022 was performed in the Facilities assessment of PR3016700.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d407bef-bd47-4160-8aa8-0066aa6eae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_conclusion = \"\"\"5.6. CONCLUSION ON THE ROOT CAUSE INVESTIGATION\n",
    "The root cause investigation demonstrated that:\n",
    "The final conductivity measured in-line for the CIP cycle of T22 performed on 12 September 2022 was satisfactory;\n",
    "NaOH 29% is the cleaning agent used for CIP of T22 tank. No carbon in that composition;  The TOC of WFI loops C102 and C104 was NMT 500 ppb;  T22 tank as well as the CIP station 638-7 were operational and the integrity of these equipments was ensured during the EM sampling;\n",
    "The EM sampling was performed according to the standard operating procedures (within 48 hours after the CIP cycle) and no activities were reported between CIP and EM sampling;\n",
    "The QC laboratory testing analysis was satisfactory and no testing error was identified;  No change impacting T22 and /or CIP station 638-7 were implemented in the period of 11 August 2022 (last satisfactory EM testing) till 21 September 2022 (re-test due to PR3016700);  The revision of maintenance activities (from 11 August 2022 to 21 September 2022) demonstrated that no maintenance Work Order had impact on this OOL TOC;\n",
    "No other non-conformance events concerning T22 were reported;  LAL, pH, conductivity and Bioburden results obtained for TANK_T22 taken on 12 September 2022 were satisfactory;\n",
    "No assignable root cause was identified in the investigation of PR3016700.\n",
    "Nonetheless, deviation PR3060455 has been opened on 14 October 2022 for an atypical trend in the occurrence OoL TOC observed for EM samples. This PR is still under investigation at the time.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a53b13-bcd1-4a45-bdf8-630094930c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rc = [rc_00, rc_CIP, rc_sampling, rc_testing, rc_material_machine_equipement, rc_rawmaterials, rc_manMethod, rc_conform, rc_conclusion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075690ce-4858-456c-84f3-405beaa578d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_full_txt = \"\\n\".join(list_rc)\n",
    "print(rc_full_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94277fcf-492c-4c10-b816-f5f980588e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = PromptTemplate(template=TEMPLATE_SUMMARY_CHAPTER_STR, input_variables=[\"context\"])\n",
    "llm_chain_summary = LLMChain(prompt=prompt_summary, llm=llm, verbose=True)\n",
    "answer_round = llm_chain_summary.run({'context':descr_event})\n",
    "print(answer_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd901a-411b-4ca6-a381-a6b7a69e56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_chain.run(({'docs':descr_event})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca29f9d-57c1-4285-af68-cf885256eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce template and chain\n",
    "reduce_template = \"\"\"<s>[INST] The following is set of summaries from the report:\n",
    "\n",
    "{doc_summaries}\n",
    "\n",
    "Based on this, distill it into a final, consolidated summary of the main points. The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information. [/INST]\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9e25d-b297-46e0-8012-586d10056436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, LLMChain, ReduceDocumentsChain, StuffDocumentsChain\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=8000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "map_reduce_chain_summary = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=llm_chain_summary,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"context\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18db9e-7d1f-4002-81bb-a765ec97e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "#text_splitter = RecursiveCharacterTextSplitter(\n",
    "#    chunk_size=1024, chunk_overlap=0\n",
    "#)\n",
    "\n",
    "documents = list()\n",
    "for doc in list_rc:\n",
    "    documents.append(Document(page_content=doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab7eb6-2b62-4e87-85d7-284d19cea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_docs = text_splitter.split_documents(docs)\n",
    "#split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853e439-5307-4c44-a465-27f84b87dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = map_reduce_chain.__call__(documents, return_only_outputs=True)\n",
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f8e19-cfab-4446-8b2c-c01d6b19cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in result['intermediate_steps']:\n",
    "    print(f'**** \\n {res} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07dfad-9507-4195-ac76-c5e12db84c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = map_reduce_chain_summary.__call__(documents, return_only_outputs=True)\n",
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec71786a-13b4-479e-9289-9e4a6131ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### QUERIES ########\n",
    "queries_desc = [\n",
    "    'Specify the violated governing procedure',\n",
    "    #'What are the specific requirements of the violated governing procedure that were not met ?',\n",
    "    'Which requirements of the violated governing procedure were not met ?',\n",
    "    'Who discovered that the governing procedure was violated ?',\n",
    "    'When was the violated governing procedure discovered ?',\n",
    "    'How was the violated governing procedure discovered ?',\n",
    "    'When did the deviation occur ?',\n",
    "    'Who was involved in the deviation ?',\n",
    "    'What are the potentially impacted elements (e.g : lots) due to the violation of the governing procedure ?'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a76fe6-f592-4416-9564-ceb9ef97abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_immediate_correction = [\n",
    "    'Were the operations stopped when deviation was discovered ?',\n",
    "    'If the operations were stopped, explain why.',\n",
    "    'If the operations were not stopped, explain which immediate correction were implemented to allow operations to continue.',\n",
    "    'When was the deviation escalated ?',\n",
    "    'Who was the deviation escalated to ? Two names are expected, one business representative and one quality representative.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0ac7d-5383-42f4-b9f2-3fdb075f681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_rc_analysis = [\n",
    "    'Detail the methodology and tools used for the root cause investigation.',\n",
    "    'List the potential causes that were evaluated.',\n",
    "    'For each potential causes, what are the rationale that led to selecting or not each cause as the root cause ?',\n",
    "    #'If a root cause was identified, is it a recurring root cause ?',\n",
    "    #'If a root cause was identified is recurring, explain why it has repeated.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec830ade-598b-490d-9f58-72420fdecb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_INSTR_CHAT_STR = (\n",
    "    \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
    "\n",
    "{context_str}\n",
    "\n",
    "<Chat history for additional context>\n",
    "{previous_str}\n",
    "\n",
    "Based on this, {query_str} [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1423f4dd-f850-496b-8247-0c9402ecd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_SUMMARY_STR = (\n",
    "    \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context. Use the following pieces of conversation to write a summary according to the instructions provided at the end.\n",
    "\n",
    "{chat_str}\n",
    "Based on this, write a comprehensive and well-structured summary written in a professional tone suitable for an academic audience. The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information. [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13718a8-f33e-43cd-a947-f78df0e87b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_history(previous_hist, current_q, current_a):\n",
    "    new_histo = f'Human: {current_q}\\nAssistant:{current_a}'\n",
    "    if previous_hist:\n",
    "        new_histo = f'{previous_hist}\\n{new_histo}'\n",
    "    else:\n",
    "        pass\n",
    "    return new_histo\n",
    "\n",
    "def create_chat_from_list(chat_list):\n",
    "    chat_str = ''\n",
    "\n",
    "    for q, a in chat_list:\n",
    "        chat_str += f'Human: {q}\\nAssistant:{a}\\n'\n",
    "\n",
    "    return chat_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce4bab3-22d2-41f0-b41e-9524e40ada68",
   "metadata": {},
   "outputs": [],
   "source": [
    " ###### HISTORY CHAT #######\n",
    "history_chat = list()\n",
    "histo_str = ''\n",
    "\n",
    "###### RELEVANT DOC #######\n",
    "#current_doc = documents[2].page_content\n",
    "\n",
    "#current_doc = documents[4].page_content + '\\n\\n' + documents[5].page_content + '\\n\\n' + documents[20].page_content\n",
    "\n",
    "#current_doc = rc_full_txt\n",
    "\n",
    "current_doc = descr_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc4e5c2f-aaad-4d20-afcf-b47f289ae323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n"
     ]
    }
   ],
   "source": [
    "print(current_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef2c3e7-2112-4126-89de-0f93d42195c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_instruct = PromptTemplate(template=TEMPLATE_INSTR_CHAT_STR, input_variables=[\"context_str\", \"previous_str\", \"query_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15bf4234-2875-4a36-8736-d2345ded6f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain_instruct = LLMChain(prompt=prompt_instruct, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b84e87e-8bf3-45e0-bc53-55bb03ffade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "\n",
      "\n",
      "Based on this, Specify the violated governing procedure [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "\n",
      "Based on this, Which requirements of the violated governing procedure were not met ? [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    85 runs   (    0.07 ms per token, 14070.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1749.08 ms /   634 tokens (    2.76 ms per token,   362.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1939.63 ms /    84 runs   (   23.09 ms per token,    43.31 tokens per second)\n",
      "llama_print_timings:       total time =    3803.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Which requirements of the violated governing procedure were not met ?\n",
      "Assistant: The requirement of the violated governing procedure SOP-048762 that was not met is the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "\n",
      "Based on this, Who discovered that the governing procedure was violated ? [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       6.59 ms /    89 runs   (    0.07 ms per token, 13501.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     404.43 ms /   120 tokens (    3.37 ms per token,   296.72 tokens per second)\n",
      "llama_print_timings:        eval time =    2061.25 ms /    88 runs   (   23.42 ms per token,    42.69 tokens per second)\n",
      "llama_print_timings:       total time =    2581.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Which requirements of the violated governing procedure were not met ?\n",
      "Assistant: The requirement of the violated governing procedure SOP-048762 that was not met is the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Who discovered that the governing procedure was violated ?\n",
      "Assistant: The discovery of the violation of the governing procedure was made during routine Environmental Monitoring (EM) testing on TANK_T22.\n",
      "\n",
      "Based on this, When was the violated governing procedure discovered ? [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       2.38 ms /    29 runs   (    0.08 ms per token, 12205.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     422.13 ms /   127 tokens (    3.32 ms per token,   300.85 tokens per second)\n",
      "llama_print_timings:        eval time =     654.90 ms /    28 runs   (   23.39 ms per token,    42.75 tokens per second)\n",
      "llama_print_timings:       total time =    1113.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Which requirements of the violated governing procedure were not met ?\n",
      "Assistant: The requirement of the violated governing procedure SOP-048762 that was not met is the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Who discovered that the governing procedure was violated ?\n",
      "Assistant: The discovery of the violation of the governing procedure was made during routine Environmental Monitoring (EM) testing on TANK_T22.\n",
      "Human: When was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK_T22 on 15 September 2022.\n",
      "\n",
      "Based on this, How was the violated governing procedure discovered ? [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       3.75 ms /    44 runs   (    0.09 ms per token, 11749.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     206.62 ms /    64 tokens (    3.23 ms per token,   309.75 tokens per second)\n",
      "llama_print_timings:        eval time =    1011.29 ms /    43 runs   (   23.52 ms per token,    42.52 tokens per second)\n",
      "llama_print_timings:       total time =    1275.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Which requirements of the violated governing procedure were not met ?\n",
      "Assistant: The requirement of the violated governing procedure SOP-048762 that was not met is the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Who discovered that the governing procedure was violated ?\n",
      "Assistant: The discovery of the violation of the governing procedure was made during routine Environmental Monitoring (EM) testing on TANK_T22.\n",
      "Human: When was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK_T22 on 15 September 2022.\n",
      "Human: How was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure, SOP-048762, was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22 on 15 September 2022.\n",
      "\n",
      "Based on this, When did the deviation occur ? [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       4.03 ms /    46 runs   (    0.09 ms per token, 11411.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     292.22 ms /    78 tokens (    3.75 ms per token,   266.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1063.55 ms /    45 runs   (   23.63 ms per token,    42.31 tokens per second)\n",
      "llama_print_timings:       total time =    1415.78 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Which requirements of the violated governing procedure were not met ?\n",
      "Assistant: The requirement of the violated governing procedure SOP-048762 that was not met is the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Who discovered that the governing procedure was violated ?\n",
      "Assistant: The discovery of the violation of the governing procedure was made during routine Environmental Monitoring (EM) testing on TANK_T22.\n",
      "Human: When was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK_T22 on 15 September 2022.\n",
      "Human: How was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure, SOP-048762, was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22 on 15 September 2022.\n",
      "Human: When did the deviation occur ?\n",
      "Assistant: Based on the provided context, it appears that the deviation occurred on 15 September 2022 when the violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22.\n",
      "\n",
      "Based on this, Who was involved in the deviation ? [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    58 runs   (    0.08 ms per token, 12736.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     296.53 ms /    78 tokens (    3.80 ms per token,   263.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1354.17 ms /    57 runs   (   23.76 ms per token,    42.09 tokens per second)\n",
      "llama_print_timings:       total time =    1725.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not reference any given instructions or context. If you don't know the answer, just say that you don't know, don't try to make up an answer. Following is a piece of context:\n",
      "\n",
      "2. DESCRIPTION OF THE EVENT\n",
      "The details upon the Out of Limit (OOL) event initiated for T22 Tank due to routine Environmental \n",
      "Monitoring (EM) are as indicated in Table 1. \n",
      "PR# Description Sample date Discovery Date Results \n",
      "3016700 OOL TOC 12 September 2022 15 September 2022 505 ppb\n",
      "Table 1 Event description\n",
      "The Total Organic Carbon (TOC) testing was performed according to SOP-015413 and results did \n",
      "not meet the acceptance criteria as defined in SOP-048762 (NMT 500 ppb). \n",
      "All others testing performed on EM samples, i.e. pH, conductivity, LAL and bioburden were within \n",
      "the specified limits (see Table 2).\n",
      "DIW \n",
      "pre-rinsing\n",
      "Alkaline \n",
      "Cleaning\n",
      "WFI \n",
      "rinsing\n",
      "\n",
      "Test Unit Result Acceptance criteria\n",
      "pH - 5.6 5.0 – 7.0\n",
      "Conductivity µS/cm 1.1 See SOP-026359\n",
      "TOC ppb 505 ≤ 500\n",
      "LAL EU/ml 0.005 ≤ 0.25\n",
      "Bioburden Cfu/10 ml 0 < 1 \n",
      "Table 2 Results of testing performed on TANK_T22 samples taken on 12 September 2022\n",
      "(ref.: NWA database). \n",
      "The previous EM testing of TANK_T22 was performed on 11 August 2022 and all the results \n",
      "obtained were satisfactory.\n",
      "The EM re-testing for TOC was performed on 21 September 2022. The TOC (50 ppb) result obtained \n",
      "was satisfactory.\n",
      " \n",
      "T22 is a buffer tank in Purification L2 Post Nano area (R605), and is used during the manufacturing \n",
      "of Cuvitru to prepare the dialysis buffer and the NaCl buffer used to clean the UF605.\n",
      "No lots were impacted by this event, as concluded in the impact assessment (see Evaluation \n",
      "PR3016700 attached in TW8).\n",
      "\n",
      "<Chat history for additional context>\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Which requirements of the violated governing procedure were not met ?\n",
      "Assistant: The requirement of the violated governing procedure SOP-048762 that was not met is the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Who discovered that the governing procedure was violated ?\n",
      "Assistant: The discovery of the violation of the governing procedure was made during routine Environmental Monitoring (EM) testing on TANK_T22.\n",
      "Human: When was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK_T22 on 15 September 2022.\n",
      "Human: How was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure, SOP-048762, was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22 on 15 September 2022.\n",
      "Human: When did the deviation occur ?\n",
      "Assistant: Based on the provided context, it appears that the deviation occurred on 15 September 2022 when the violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22.\n",
      "Human: Who was involved in the deviation ?\n",
      "Assistant: Based on the provided context, it appears that the deviation occurred when the violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22. However, it is not specified who was involved in the deviation.\n",
      "\n",
      "Based on this, What are the potentially impacted elements (e.g : lots) due to the violation of the governing procedure ? [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       4.43 ms /    62 runs   (    0.07 ms per token, 14008.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     316.24 ms /    89 tokens (    3.55 ms per token,   281.43 tokens per second)\n",
      "llama_print_timings:        eval time =    1472.79 ms /    61 runs   (   24.14 ms per token,    41.42 tokens per second)\n",
      "llama_print_timings:       total time =    1867.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    64 runs   (    0.07 ms per token, 13725.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     409.09 ms /   109 tokens (    3.75 ms per token,   266.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1539.47 ms /    63 runs   (   24.44 ms per token,    40.92 tokens per second)\n",
      "llama_print_timings:       total time =    2030.06 ms\n"
     ]
    }
   ],
   "source": [
    "queries = queries_desc\n",
    "for q in queries:\n",
    "    answer_round = llm_chain_instruct.run({'context_str':current_doc, 'previous_str': histo_str, 'query_str':q})\n",
    "    histo_str = build_history(histo_str, q, answer_round)\n",
    "    history_chat.append((q, answer_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca948cda-ca4b-42b9-a3ce-4d075771593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context. Use the following pieces of conversation to write a summary according to the instructions provided at the end.\n",
      "\n",
      "Human: Specify the violated governing procedure\n",
      "Assistant: The violated governing procedure is SOP-048762, which defines the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Which requirements of the violated governing procedure were not met ?\n",
      "Assistant: The requirement of the violated governing procedure SOP-048762 that was not met is the acceptance criteria for Total Organic Carbon (TOC) testing. The results of the TOC testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb.\n",
      "Human: Who discovered that the governing procedure was violated ?\n",
      "Assistant: The discovery of the violation of the governing procedure was made during routine Environmental Monitoring (EM) testing on TANK_T22.\n",
      "Human: When was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK_T22 on 15 September 2022.\n",
      "Human: How was the violated governing procedure discovered ?\n",
      "Assistant: The violated governing procedure, SOP-048762, was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22 on 15 September 2022.\n",
      "Human: When did the deviation occur ?\n",
      "Assistant: Based on the provided context, it appears that the deviation occurred on 15 September 2022 when the violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22.\n",
      "Human: Who was involved in the deviation ?\n",
      "Assistant: Based on the provided context, it appears that the deviation occurred when the violated governing procedure SOP-048762 was discovered during routine Environmental Monitoring (EM) testing on TANK\\_T22. However, it is not specified who was involved in the deviation.\n",
      "Human: What are the potentially impacted elements (e.g : lots) due to the violation of the governing procedure ?\n",
      "Assistant: Based on the provided context, it appears that no lots were impacted by the violation of the governing procedure SOP-048762. The impact assessment concluded that no lots were affected by this event, as stated in Evaluation PR3016700 attached in TW8.\n",
      "\n",
      "Based on this, write a comprehensive and well-structured summary written in a professional tone suitable for an academic audience. The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information. [/INST]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " In summary, on September 15, 2022, during routine Environmental Monitoring (EM) testing on TANK_T22, a violation of governing procedure SOP-048762 was discovered. The results of the Total Organic Carbon (TOC) testing performed on TANK_T22 did not meet the acceptance criteria as defined in this procedure, with a result of 505 ppb exceeding the NMT limit of 500 ppb. The impact assessment concluded that no lots were affected by this event, as stated in Evaluation PR3016700 attached in TW8. It is not specified who was involved in the deviation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1385.98 ms\n",
      "llama_print_timings:      sample time =      10.85 ms /   152 runs   (    0.07 ms per token, 14010.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     554.26 ms /   120 tokens (    4.62 ms per token,   216.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3600.76 ms /   151 runs   (   23.85 ms per token,    41.94 tokens per second)\n",
      "llama_print_timings:       total time =    4352.57 ms\n"
     ]
    }
   ],
   "source": [
    "summary_prompt = PromptTemplate(template=TEMPLATE_SUMMARY_STR, input_variables=[\"chat_str\"])\n",
    "llm_chain_summary = LLMChain(prompt=summary_prompt, llm=llm, verbose=True)\n",
    "summary_dev = llm_chain_summary.run({'chat_str': create_chat_from_list(history_chat)})\n",
    "print(summary_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc6731-1533-4ad5-9c6a-1cf1cf0e3e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_scope = \"What is the purpose of this report ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0449b2a-6048-42ba-833e-322533e99cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_PURPOSE_STR = (\n",
    "    \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible and follow ALL given instructions. Do not speculate or make up information. Do not reference any given instructions or context. The following is a part of report:\n",
    "\n",
    "{context}\n",
    "\n",
    "Based on this, written in a professional tone suitable for an academic audience, answer this query : {query_str} [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3a700-af51-4ccf-b0bd-f62c10578b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_query_purpose = PromptTemplate(template=TEMPLATE_PURPOSE_STR, input_variables=[\"context\", \"query_str\"])\n",
    "llm_chain_extract = LLMChain(prompt=prompt_query_purpose, llm=llm, verbose=True)\n",
    "answer_round = llm_chain_extract.run({'context':chapter_, 'query_str': queries_scope})\n",
    "print(answer_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5bc7bd-bb38-4aa4-98a1-9aa36ac52bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
